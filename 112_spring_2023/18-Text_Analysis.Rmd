```{r 18_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(ggmap)
library(lubridate)
library(rvest)
library(tidytext)
library(wordcloud)
library(jsonlite)
library(RCurl)
library(XML)
library(RJSONIO)
```

# Text Analysis  {-}

## Learning Goals {-}

- Understand the analysis process of decomposing text into tokens and considering word/token frequency
- Develop comfort in comparing the text across multiple documents in terms of tf-idf and log odds ratio
- Develop comfort in using lexicons to perform sentiment analysis on a document of text


## Introduction to Text Analysis in R {-}

We have seen how to manipulate strings with regular expressions. Here, we examine how to analyze longer text documents. Text refers to information composed primarily of words: song lyrics, Tweets, news articles, novels, Wikipedia articles, online forums, and countless other resources.

In `R` and most other programming languages, text is stored in strings of characters.

There are a variety of common ways to get strings containing the text we want to analyze.

### Text Acquisition {-}

**String Literals**

It may be natural to start by declaring an `R` variable that holds a string. Let's consider the [U.S. Declaration of Independence](https://en.wikipedia.org/wiki/United_States_Declaration_of_Independence).
Here's an `R` variable that contains one of the most memorable sentences in the Declaration of Independence:

```{r}
us_dec_sentence <- "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness."

# Show the number of characters in the sentence.
nchar(us_dec_sentence)

# Show the sentence itself.
us_dec_sentence
```

Unfortunately, creating literal string variables like this can become unwieldy for larger texts, or collections of multiple texts.

Using this technique, your `R` program would be narrowly written to analyze [*hard-coded*](https://en.wikipedia.org/wiki/Hard_coding) string variables, and defining those string variables may take up the vast majority of our program's source code, making it difficult to read.

We will discuss two more flexible ways of getting textual data: reading a `.TXT` file and accessing a web API.

#### Reading `.txt` Files {-}

The simplest file format for text is a `.TXT` (or `.txt`) file. A `.txt` file contains raw textual data. You can find `.txt` files by using Google's `filetype:` search filter.
Go to http://google.com and type `filetype:txt declaration of independence` in the search box.

In the results you should see many `.txt` files containing the U.S. Declaration of Independence.

For example, https://infamous.net/documents/declaration-of-independence.txt. We can read this `.txt` file into `R` as a string using the `readr` package.^[Instead of reading the file directly from the internet, it is often a good idea to first save it to your working directory through your browser, and then read it locally. The benefits of this include having the data backed-up in case the website changes, and being able to run your code if you are offline. The drawback is that if the website is updated and you actually want those changes to be reflected in your text analysis, you'll have to download new versions periodically.] 

Because the text is so large, we use the `strtrim` function to only show the first 500 characters of the text.

```{r}
library(readr)
us_dec <- read_file("https://infamous.net/documents/declaration-of-independence.txt")
nchar(us_dec)
strtrim(us_dec, 500)
```
Notice all those `\n` sequences that appear in the string. 

These are *newline* characters that denote the end of a line.

There are a few other [special characters](https://en.wikipedia.org/wiki/Escape_character) that you may see. For example, `'\t'` is a tab.

#### Using Web APIs {-}


When we want to analyze textual data created on websites or mobile apps such as Facebook and Twitter, we can use web APIs to gather the text. Here is one example from the largest single collection of written knowledge in human history: Wikipedia! 

The function below retrieves the text content of a Wikipedia article with a particular title. It uses Wikipedia's *Public API* to do so, which enables any computer program to interact with Wikipedia. Wikipedia's API is convenient for us because it is vast, open, and free. Don't worry if you don't follow the details of the code below.

```{r}
GetArticleText <- function(langCode, titles) {
  # Returns the text of the specified article in the specified language

  # An accumulator variable that will hold the text of each article

  # Create
  texts <- sapply(titles, function(t) {
    print(t)
    article_info <- getForm(
      paste("https://", langCode, ".wikipedia.org/w/api.php", sep = ""),
      action  = "query",
      prop = "extracts",
      format  = "json",
      explaintext = "",
      titles  = t
    )

    js <- fromJSON(article_info)
    return(js$query$pages[[1]]$extract)
  })
  return(texts)
}

# Get the text for https://en.wikipedia.org/wiki/Macalester_College,
# https://en.wikipedia.org/wiki/Carleton_College, and https://en.wikipedia.org/wiki/University_of_Minnesota in English ("en").
# We could also get the text for the Spanish article ("es"), or German article ("de")

school_wiki_titles <- c("Macalester College", "Carleton College", "University of Minnesota")
school_wiki_text <- GetArticleText("en", school_wiki_titles)

# Print out the first 500 characters of the text
strtrim(school_wiki_text, 500)
```

We'll analyze these documents further below.

### Analyzing Single Documents {-}

If we tried to make a data frame directly out of the text, it would look odd. It contains the text as a single row in a column named "text." This doesn't seem any more useful than the original string itself.

```{r}
us_dec_df <- tibble(title = "Declaration of Independence", text = us_dec)
us_dec_df
```

#### Unnesting Tokens {-}

We need to restructure the text into components that can be easily analyzed.

We will use two units of data:

- A **token** is the smallest textual information unit we wish to measure, typically a word.
- A **document** is a collection of tokens. 

For our example here, the Declaration of Independence is the document, and a word is the token. However, a document could be a tweet, a novel chapter, a Wikipedia article, or anything else that seems interesting. Other possibilities for tokens include sentences, lines, paragraphs, characters, [ngrams](https://en.wikipedia.org/wiki/N-gram), and more.^[See the help for `unnest_tokens` to learn more about options for the token.]

Later on, we will also give an example of how to perform textual analyses comparing two or more documents.

We will be using the tidy text format, which has one row for each unit of analysis. Our work will focus on word-level analysis within each document, so each row will contain a document and word.

TidyText's `unnest_tokens` function takes a data frame containing one row per document and breaks it into a data frame containing one row per token.

```{r}

tidy_us_dec <- us_dec_df %>%
  unnest_tokens(word, text)

tidy_us_dec
```

Note that because we only have one document, the initial data frame (`us_dec_df`) is just one row and the tidy text data frame (`tidy_us_dec`) has the same `title` for each row. 

Later on we will analyze more than one document and these columns can change.

We can now analyze this tidy text data frame. For example, we can determine the total number of words.

```{r}
nrow(tidy_us_dec)
```

We can also find the most frequently used words by using dplyr's `count` function, which creates a frequency table for (in our case) words: 

```{r}
# Create and display frequency count table
all_us_dec_counts <- tidy_us_dec %>%
  count(word, sort = TRUE)
all_us_dec_counts
```
We can count the rows in this data frame to determine how many different unique words appear in the document.

```{r}
nrow(all_us_dec_counts)
```

#### Stop Words {-}

Notice that the most frequent words are common words that are present in any document and not particularly descriptive of the topic of the document.
These common words are called **stop words**, and they are typically removed from textual analysis.

TidyText provides a built in set of 1,149 different stop words.
We can load the dataset and use `anti_join` to remove rows associated with words in the dataset.

```{r}
# Load stop words dataset and display it
data(stop_words)
stop_words

# Create and display frequency count table after removing stop words from the dataset
us_dec_counts <- tidy_us_dec %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)
us_dec_counts
```

**Word Clouds**

A [word cloud](https://georeferenced.wordpress.com/2013/01/15/rwordcloud/) is a visualization of the most frequent words in the dataset:

```{r, fig.width=6, fig.height=6}
library(wordcloud)

# Show a word cloud with some customized options

wordcloud(us_dec_counts$word, # column of words
  us_dec_counts$n, # column of frequencies
  scale = c(5, 0.2), # range of font sizes of words
  min.freq = 2, # minimum word frequency to show
  max.words = 200, # show the 200 most frequent words
  random.order = FALSE, # position the most popular words first
  colors = brewer.pal(8, "Dark2") # color palette
) 
```

### Comparing the text in two (or more) documents {-}

Let's now create a TidyText data frame with the three Wikipedia documents we collected above via the API. Remember that the TidyText data frame has one row for each word.

```{r}
# Create the three-row original data frame
text_df <- tibble(article = school_wiki_titles, text = school_wiki_text)
text_df
```

```{r,warning=FALSE}
# Unnest the data frame so each row corresponds to a single word in a single document.
tidy_df <- text_df %>%
  unnest_tokens(word, text)
tidy_df
```

#### Side-by-Side Word Clouds {-}

```{r}
macalester_counts <- tidy_df %>%
  filter(article == "Macalester College") %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)
macalester_counts

umn_counts <- tidy_df %>%
  filter(article == "University of Minnesota") %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)
umn_counts

carleton_counts <- tidy_df %>%
  filter(article == "Carleton College") %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)
carleton_counts
```

```{r,fig.show='hold',fig.fullwidth=TRUE,fig.width=4,warning=FALSE}
wordcloud(macalester_counts$word, macalester_counts$n,
  max.words = 200, random.order = FALSE, colors = brewer.pal(8, "Dark2")
)
wordcloud(umn_counts$word, umn_counts$n,
  max.words = 200, random.order = FALSE, colors = brewer.pal(8, "Dark2")
)
wordcloud(carleton_counts$word, carleton_counts$n,
  max.words = 200, random.order = FALSE, colors = brewer.pal(8, "Dark2")
)
```

**Brainstorm**

How do we compare multiple documents quantitatively?

```{exercise}
Brainstorm a metric for comparing the relative frequency/importance of different words in two or more documents. What factors might you account for?

```

#### Term Frequency - Inverse Document Frequency {-}

To compare the prevalence of certain words in one document relative to another document, we could just count the occurrences. However, the documents may be different lengths, meaning that many more words might occur more often in the longer document. There are different ways to account for this, but one of the most common is [term frequency - inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). 

- The *term frequency* aims to capture how frequently a word appears in each document. There are different ways to measure this, including a raw count, logarithmically scaled (1 + log of the raw count), or Boolean (either 1 or 0 depending on whether the word occurs). 
- The *inverse document frequency* aims to capture how common the word is across documents. It is 
$$\log\left(\frac{N}{|\{doc: word \in doc\}|}\right),$$
where $N$ is the number of documents, and the denominator of the fraction is the number of documents in which the selected word appears. Thus, if the word appears in all documents under consideration, the idf score is equal to log(1)=0. 
- The *td-idf score* is then the product of the term frequency and the inverse document frequency.

We'll use the `bind_tf_idf` command from the `tidytext` library. Its default measure for term frequency is the raw count of a given word divided by the total number of words in the document. Let's start by computing the thirty-five document-word pairs with the highest tf-idf scores:

```{r}
tfidf_analysis <- tidy_df %>%
  count(article, word) %>%
  bind_tf_idf(word, article, n) %>%
  arrange(desc(tf_idf))
```

```{r,echo=FALSE}
knitr::kable(tfidf_analysis[1:35, ], caption = "The thirty-five document-word pairs with the highest tf-idf scores.")
```

Here is a graphic with the same data:

```{r}
tfidf_analysis %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  top_n(35) %>%
  ggplot(aes(word, tf_idf, fill = article)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()
```  

Next, let's say we want to determine which school is the most relevant to the query "internationalism, multiculturalism, and service to society."

```{r}
target_words <- c("internationalism", "multiculturalism", "service", "society")
mission <- tfidf_analysis %>%
  filter(word %in% target_words)
```

```{r,echo=FALSE}
knitr::kable(mission)
```

#### Log Odds Ratio {-}

Another metric for comparing the frequency of different words in two documents is the log odds ratio:

$$\log\left(\frac{\left(\frac{n+1}{total+1}\right)_{\hbox{doc1}}}{\left(\frac{n+1}{total+1}\right)_{\hbox{doc2}}} \right),$$
where $n$ is the number of times the word appears and $total$ is the total number of words in the document.

```{r}
total.mac <- nrow(filter(tidy_df, article == "Macalester College"))
total.carleton <- nrow(filter(tidy_df, article == "Carleton College"))
logratios <- macalester_counts %>%
  full_join(carleton_counts, by = "word", suffix = c(".mac", ".carleton")) %>%
  replace_na(list(n.mac = 0, n.carleton = 0)) %>%
  mutate(n.total = n.mac + n.carleton) %>%
  filter(n.total >= 5) %>%
  mutate(logodds.mac = log(((n.mac + 1) / (total.mac + 1)) / ((n.carleton + 1) / (total.carleton + 1))))
```

Which words appear at roughly equal frequencies?

```{r}
logratios %>%
  arrange(abs(logodds.mac)) %>%
  head(n = 20)
```

What are the most distinctive words?

```{r,fig.width=10,fig.height=10}
logratios %>%
  group_by(logodds.mac < 0) %>%
  top_n(15, abs(logodds.mac)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logodds.mac)) %>%
  ggplot(aes(word, logodds.mac, fill = logodds.mac < 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio (Mac/Carleton)") +
  scale_fill_discrete(name = "", labels = c("Macalester", "Carleton"))
```

### Sentiment Analysis {-}

We often want to understand whether text conveys certain characteristics. For example, is Macalester's mission statement more happy, sad, or angry than that of the University of Minnesota?

A common way of doing this is by using a word dictionary that contains a list of words with the characteristics we are seeking (e.g., a list of words that are happy, sad, or angry). We can then measure how often words with each characteristic appear in the text. These word dictionaries are also called *lexicons*, and dictionaries related to emotive feelings are often called *sentiment lexicons*.

Tidy Text's `sentiments` dataset contains built-in sentiment lexicons. We can look at the structure of some of these:

```{r}
afinn <- get_sentiments("afinn")
nrc <- get_sentiments("nrc")
bing <- get_sentiments("bing")
```

```{r,echo=FALSE, fig.width=8}
knitr::kable(afinn[1:6, ], align = "ll")
knitr::kable(nrc[1:6, ], align = "ll")
knitr::kable(bing[1:6, ], align = "ll")
```

Let's take a look at the sentiments described within each lexicon:

```{r}
# Show the number of words and unique sentiments in each lexicon
afinn %>%
  summarize(num_words = n(), values = paste(sort(unique(value)), collapse = ", "))

nrc %>%
  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = ", "))

bing %>%
  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = ", "))
```


The Tidy Text book has some great background on these data sets:

> 
> The three general-purpose lexicons are
> 
> * `AFINN` from [Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),
> * `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and
> * `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).
> 
> All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The `nrc` lexicon categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the `sentiments` dataset, and tidytext provides a function `get_sentiments()` to get specific sentiment lexicons without the columns that are not used in that lexicon.

To apply these dictionaries, we need to create a Tidy Text data frame with words for each row and join it to the dictionary scores. Let's give this a try using the [Macalester Statement of Purpose and Belief](https://www.macalester.edu/about/mission/). We start by creating the Tidy Text data frame:

```{r}
# Declare a string containing the Macalester Statement of Purpose & Belief
statement <- "At Macalester College we believe that education is a fundamentally transforming experience. As a community of learners, the possibilities for this personal, social, and intellectual transformation extend to us all. We affirm the importance of the intellectual growth of the students, staff and faculty through individual and collaborative endeavor. We believe that this can best be achieved through an environment that values the diverse cultures of our world and recognizes our responsibility to provide a supportive and respectful environment for students, staff and faculty of all cultures and backgrounds.

We expect students to develop a broad understanding of the liberal arts while they are at Macalester. Students should follow a primary course of study in order to acquire an understanding of disciplinary theory and methodology; they should be able to apply their understanding of theories to address problems in the larger community. Students should develop the ability to use information and communication resources effectively, be adept at critical, analytical and logical thinking, and express themselves well in both oral and written forms. Finally, students should be prepared to take responsibility for their personal, social and intellectual choices.

We believe that the benefit of the educational experience at Macalester is the development of individuals who make informed judgments and interpretations of the broader world around them and choose actions or beliefs for which they are willing to be held accountable. We expect them to develop the ability to seek and use knowledge and experience in contexts that challenge and inform their suppositions about the world. We are committed to helping students grow intellectually and personally within an environment that models and promotes academic excellence and ethical behavior. The education a student begins at Macalester provides the basis for continuous transformation through learning and service."

# Expand this into a tidy data frame, with one row per word
tidy_df <- tibble(college = c("Macalester College"), text = statement) %>%
  unnest_tokens(word, text)

# Display the data frame and the most popular words
tidy_df

tidy_df %>%
  anti_join(stop_words) %>%
  count(word)
```

Next, we join this data frame with the lexicon. Let's use nrc. Since we don't care about words not in the lexicon, we will use an inner join.

```{r}
tidy_df %>%
  inner_join(nrc) %>%
  count(sentiment)
```

There are some odd sentiments for a mission statement (anger, disgust, fear, and negative). Let's take a look at what words are contributing to them.

```{r}
tidy_df %>%
  inner_join(nrc) %>%
  filter(sentiment %in% c("anger", "disgust", "fear", "negative")) %>%
  select(word, sentiment)
```

As you can see, word dictionaries are not perfect tools. When using them, make sure you look at the individual words contributing to the overall patterns to ensure they make sense.


### Other Interesting Questions {-}

There are all sorts of other interesting questions we can ask when analyzing texts. These include:

- How do word frequencies change over time (e.g., Twitter) or over the course of a text?
- What is the correlation between different words (or names of characters in novels)? For example, how frequently do they appear in the same section of a text, or within $K$ number of words of each other?^[Check out the `widyr` package and its `pairwise_count()` function if interested in these and similar questions.]
- How can we visualize such co-occurrences with networks?
- What "topics" are found in different documents? What word collections comprise these topics? This area is called [topic modeling](http://tidytextmining.com/topicmodeling.html). 
- Can you guess who wrote a document by analyzing its text?
- How does the structure of different languages (e.g., sentence structure, sentence length, parts-of-speech) compare? These and many other interesting questions are asked by [computational linguists](https://en.wikipedia.org/wiki/Computational_linguistics).
